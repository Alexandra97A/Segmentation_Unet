{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resnet50Layers.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d46b7c7431814e4196ad05e0ca02865d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c0442151a9ac4fc1addbb7fbc535d8b7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0358a40c85a946d987c97cff6fa9e52a","IPY_MODEL_cfb5c67e28c24f8b954ebc608bb2cc3c"]},"model_module_version":"1.5.0"},"c0442151a9ac4fc1addbb7fbc535d8b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"0358a40c85a946d987c97cff6fa9e52a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_049415f12b9b4531abbd59e96627e565","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102530333,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102530333,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4a57a837d6ec469a8297fd84cd3af9af"},"model_module_version":"1.5.0"},"cfb5c67e28c24f8b954ebc608bb2cc3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0926cfdacc374fd4b59ad486100063a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:01&lt;00:00, 96.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f24676d06cdf40f6bb350f9565026966"},"model_module_version":"1.5.0"},"049415f12b9b4531abbd59e96627e565":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"4a57a837d6ec469a8297fd84cd3af9af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"0926cfdacc374fd4b59ad486100063a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"f24676d06cdf40f6bb350f9565026966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["d46b7c7431814e4196ad05e0ca02865d","c0442151a9ac4fc1addbb7fbc535d8b7","0358a40c85a946d987c97cff6fa9e52a","cfb5c67e28c24f8b954ebc608bb2cc3c","049415f12b9b4531abbd59e96627e565","4a57a837d6ec469a8297fd84cd3af9af","0926cfdacc374fd4b59ad486100063a3","f24676d06cdf40f6bb350f9565026966"]},"id":"IXhumyKzYL7_","outputId":"5afc7129-2ef3-47ba-b97c-e28109820c60"},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","resnet = torchvision.models.resnet.resnet50(pretrained=True)\n","\n","\n","class ConvBlock(nn.Module):\n","    \"\"\"\n","    Helper module that consists of a Conv -> BN -> ReLU\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","        self.with_nonlinearity = with_nonlinearity\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        if self.with_nonlinearity:\n","            x = self.relu(x)\n","        return x\n","\n","\n","class Bridge(nn.Module):\n","    \"\"\"\n","    This is the middle layer of the UNet which just consists of some\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.bridge = nn.Sequential(\n","            ConvBlock(in_channels, out_channels),\n","            ConvBlock(out_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.bridge(x)\n","\n","\n","class UpBlockForUNetWithResNet50(nn.Module):\n","    \"\"\"\n","    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n","                 upsampling_method=\"conv_transpose\"):\n","        super().__init__()\n","\n","        if up_conv_in_channels == None:\n","            up_conv_in_channels = in_channels\n","        if up_conv_out_channels == None:\n","            up_conv_out_channels = out_channels\n","\n","        if upsampling_method == \"conv_transpose\":\n","            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n","        elif upsampling_method == \"bilinear\":\n","            self.upsample = nn.Sequential(\n","                nn.Upsample(mode='bilinear', scale_factor=2),\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n","            )\n","        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n","        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n","\n","    def forward(self, up_x, down_x):\n","        \"\"\"\n","        :param up_x: this is the output from the previous up block\n","        :param down_x: this is the output from the down block\n","        :return: upsampled feature map\n","        \"\"\"\n","        x = self.upsample(up_x)\n","        x = torch.cat([x, down_x], 1)\n","        x = self.conv_block_1(x)\n","        x = self.conv_block_2(x)\n","        return x\n","\n","\n","class UNetWithResnet50Encoder(nn.Module):\n","    DEPTH = 6\n","\n","    def __init__(self, n_classes=2):\n","        super().__init__()\n","        resnet = torchvision.models.resnet.resnet50(pretrained=True)\n","        down_blocks = []\n","        up_blocks = []\n","        self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n","        self.input_pool = list(resnet.children())[3]\n","        for bottleneck in list(resnet.children()):\n","            if isinstance(bottleneck, nn.Sequential):\n","                down_blocks.append(bottleneck)\n","        self.down_blocks = nn.ModuleList(down_blocks)\n","        self.bridge = Bridge(2048, 2048)\n","        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n","        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n","        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n","        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n","                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n","        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n","                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n","\n","        self.up_blocks = nn.ModuleList(up_blocks)\n","\n","        self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1)\n","\n","    def forward(self, x, with_output_feature_map=False):\n","        pre_pools = dict()\n","        pre_pools[f\"layer_0\"] = x\n","        x = self.input_block(x)\n","        pre_pools[f\"layer_1\"] = x\n","        x = self.input_pool(x)\n","\n","        for i, block in enumerate(self.down_blocks, 2):\n","            x = block(x)\n","            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n","                continue\n","            pre_pools[f\"layer_{i}\"] = x\n","\n","        x = self.bridge(x)\n","\n","        for i, block in enumerate(self.up_blocks, 1):\n","            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n","            x = block(x, pre_pools[key])\n","        output_feature_map = x\n","        x = self.out(x)\n","        del pre_pools\n","        if with_output_feature_map:\n","            return x, output_feature_map\n","        else:\n","            return x\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d46b7c7431814e4196ad05e0ca02865d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102530333.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OqPNcQdCYTP_"},"source":["LEARNING_RATE = 5e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Learning rate is \", LEARNING_RATE)\n","import torch.optim as optim\n","device=DEVICE\n","filename=''\n","BATCH_SIZE =4\n","NUM_EPOCHS = 500\n","NUM_WORKERS = 1\n","UnetDepth=5\n","model = UNet(n_classes=1, padding=True,batch_norm=True, up_mode='upsample',depth=UnetDepth).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","PIN_MEMORY = True\n","LOAD_MODEL = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgZrKC8vZNc0","outputId":"d10a016b-4e8a-43ac-d471-685d520c77fb"},"source":["UNetWithResnet50Encoder()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["UNetWithResnet50Encoder(\n","  (input_block): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (input_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (down_blocks): ModuleList(\n","    (0): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (bridge): Bridge(\n","    (bridge): Sequential(\n","      (0): ConvBlock(\n","        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","      (1): ConvBlock(\n","        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","    )\n","  )\n","  (up_blocks): ModuleList(\n","    (0): UpBlockForUNetWithResNet50(\n","      (upsample): ConvTranspose2d(2048, 1024, kernel_size=(2, 2), stride=(2, 2))\n","      (conv_block_1): ConvBlock(\n","        (conv): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","      (conv_block_2): ConvBlock(\n","        (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","    )\n","    (1): UpBlockForUNetWithResNet50(\n","      (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n","      (conv_block_1): ConvBlock(\n","        (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","      (conv_block_2): ConvBlock(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","    )\n","    (2): UpBlockForUNetWithResNet50(\n","      (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (conv_block_1): ConvBlock(\n","        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","      (conv_block_2): ConvBlock(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","    )\n","    (3): UpBlockForUNetWithResNet50(\n","      (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n","      (conv_block_1): ConvBlock(\n","        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","      (conv_block_2): ConvBlock(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","    )\n","    (4): UpBlockForUNetWithResNet50(\n","      (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n","      (conv_block_1): ConvBlock(\n","        (conv): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","      (conv_block_2): ConvBlock(\n","        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","      )\n","    )\n","  )\n","  (out): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5DuR244CRTvx","outputId":"abeb8fda-e468-4914-d67a-f8b3b965cb28"},"source":["!pip install --upgrade --force-reinstall --no-deps albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting albumentations\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/2fa0ec5e0c04c410cbb54dd79910afa884409440653aa4688654e6497e2a/albumentations-1.0.2-py3-none-any.whl (98kB)\n","\r\u001b[K     |███▎                            | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20kB 22.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30kB 27.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 40kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 51kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 71kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 81kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 8.1MB/s \n","\u001b[?25hInstalling collected packages: albumentations\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-1.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJ0_mxMYRaaZ","outputId":"64cf1f83-b921-4ab3-ecdb-a3acab99f209"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yuj0qxfXRb_e"},"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms.functional as TF\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from tqdm import tqdm\n","import numpy as np\n","from os import path\n","from skimage.io import imread\n","from torch.utils import data\n","import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKTS-DsVz1az"},"source":["import torch\n","def training_function(loader, model, optimizer, loss_fn):\n","  loop = tqdm(loader)\n","  trainingLossSum=0\n","  for batch_idx, (data, targets) in enumerate(loop):\n","      data = data.to(device=DEVICE)\n","      targets = targets.float().to(device=DEVICE)\n","      predictions = model(data)#.permute(0, 1, 3,2)\n","      train_loss = loss_fn(predictions, targets)\n","      trainingLossSum+=train_loss                  \n","      optimizer.zero_grad()\n","      train_loss.backward()\n","      optimizer.step()\n","      # update tqdm loop\n","      loop.set_postfix(loss=train_loss.item())\n","  return trainingLossSum/len(loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4GasHsQS4Jf"},"source":["def validating_function(loader,model,optimizer,loss_fn):\n","  loop = tqdm(loader)\n","  valLossSum=0\n","  for batch_idx, (data, targets) in enumerate(loop):\n","      data = data.to(device=DEVICE)\n","      targets = targets.float().to(device=DEVICE)\n","      # print('targets shape is:',targets.size())\n","      with torch.no_grad(): # <1> dont build graph on validation\n","              predictions = model(data)\n","              val_loss = loss_fn(predictions, targets)\n","              valLossSum+=  val_loss   \n","              assert val_loss.requires_grad == False # <2> checks that our output requires_grad\n","              # args are forced to False inside this block\n","               \n","      optimizer.zero_grad()\n","      optimizer.step()\n","       # update tqdm loop\n","      loop.set_postfix(loss=val_loss.item())\n","  return valLossSum/len(loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uGdIj9jkS58T"},"source":["Preparing data"]},{"cell_type":"code","metadata":{"id":"ZInzzMftS8TR"},"source":["from torchvision import transforms\n","to_tensor = transforms.ToTensor()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lW6koDG4TOm5"},"source":["import os\n","from torch.utils.data import Dataset\n","import numpy as np\n","from os import path\n","from PIL import Image\n","import cv2\n","\n","from PIL import Image\n","\n","import torch\n","# from skimage.io import imread\n","# from torch.utils import data\n","\n","def replaceInputName(mask_path,diseaseType):\n","  result = mask_path.index('IDRiD')\n","  newString=''\n","\n","  if diseaseType==1:  #for Hard exudates\n","    newString= '_EX'\n","  elif diseaseType==2: #for hemorrages\n","    newString= '_HE'\n","  elif diseaseType==3:  \n","    newString= '_SE'\n","  else:\n","    newString= '_MA'\n","\n","  toReplace= mask_path[result:result+8]\n","  mask_path=mask_path.replace(toReplace, toReplace+newString)\n","  mask_path=mask_path.replace('images','groundtruths')\n","  return mask_path\n","\n","class CarvanaDataset(Dataset):\n","   \n","    def __init__(self, diseaseType,image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","        self.diseaseType= diseaseType\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = replaceInputName(img_path,diseaseType)\n","\n","\n","        image= cv2.imread(img_path)\n","        image = np.array(image, dtype=np.float32)\n","        \n","        mask = cv2.imread(mask_path)\n","        mask = mask[:,:,2]\n","        mask[mask == 255.0] = 1.0\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","        return image, mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TP1mibRhTQWi"},"source":["LEARNING_RATE = 5e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Learning rate is \", LEARNING_RATE)\n","import torch.optim as optim\n","device=DEVICE\n","filename=''\n","BATCH_SIZE =4\n","NUM_EPOCHS = 500\n","NUM_WORKERS = 1\n","UnetDepth=5\n","IMAGE_HEIGHT = 512 \n","IMAGE_WIDTH = 512\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","\n"," \n","PIN_MEMORY = True\n","LOAD_MODEL = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtdQSc20TUT2"},"source":["lossFunction=4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKHijGewTV5_"},"source":["LEARNING_RATE=0\n","if lossFunction==1:\n","    LEARNING_RATE = 5e-5\n","    ALPHA = 0.5\n","    BETA = 0.5\n","    # https://arxiv.org/abs/1706.05721\n","    #optimise segmentation on imbalanced medical datasets\n","    # in the case of α=β=0.5 the Tversky index simplifies to be the same as the Dice coefficient, which is also equal to the F1 score\n","    #modify beta increasing, so FN are punished harder\n","    class TverskyLoss(nn.Module):\n","        def __init__(self, weight=None, size_average=True):\n","            super(TverskyLoss, self).__init__()\n","\n","        def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n","            \n","            #comment out if your model contains a sigmoid or equivalent activation layer\n","            inputs = F.sigmoid(inputs)       \n","            \n","            #flatten label and prediction tensors\n","            inputs = inputs.view(-1)\n","            targets = targets.view(-1)\n","            \n","            #True Positives, False Positives & False Negatives\n","            TP = (inputs * targets).sum()    \n","            FP = ((1-targets) * inputs).sum()\n","            FN = (targets * (1-inputs)).sum()\n","          \n","            Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n","            \n","            return 1 - Tversky\n","    loss_fn = TverskyLoss().to(DEVICE)\n","    filename=\"TverskyLossModel\"+str(ALPHA)+str(BETA)+\"resnetEbaneo\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","    print(\"TverskyLoss selected\")\n","    print(filename)\n","elif lossFunction==2:\n","      LEARNING_RATE = 5e-5\n","      #combo loss :  https://arxiv.org/abs/1805.02798\n","      #Combo loss is a combination of Dice Loss and a modified Cross-Entropy function that,\n","      # like Tversky loss, has additional constants which penalise either false positives or false negatives more respectively.\n","      ##PyTorch\n","      ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\n","      CE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n","      e=1e-7\n","      class ComboLoss(nn.Module):\n","        def __init__(self, weight=None, size_average=True):\n","            super(ComboLoss, self).__init__()\n","\n","        def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n","            \n","            #flatten label and prediction tensors\n","            inputs = inputs.view(-1)\n","            targets = targets.view(-1)\n","            \n","            #True Positives, False Positives & False Negatives\n","            intersection = (inputs * targets).sum()    \n","            dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n","            \n","            inputs = torch.clamp(inputs, e, 1.0 - e)       \n","            out = - (ALPHA * ((targets * torch.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * torch.log(1.0 - inputs))))\n","            weighted_ce = out.mean(-1)\n","            combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n","            \n","            return combo\n","      loss_fn = ComboLoss().to(DEVICE)\n","      filename=\"ComboLossModel\"+str(alpha)+str(1-alpha)+\"resnetEbaneo\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","      print(\"Combo loss selected\")\n","      print(filename)\n","elif lossFunction == 3:\n","      LEARNING_RATE = 5e-5\n","      #Focal Loss\n","      #for combatting extremely imbalanced datasets https://arxiv.org/abs/1708.02002\n","\n","      ALPHA = 0.8 #DONT MODIFY\n","      GAMMA = 2  #DONT MODIFY\n","\n","      class FocalLoss(nn.Module):\n","          def __init__(self, weight=None, size_average=True):\n","              super(FocalLoss, self).__init__()\n","\n","          def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n","              \n","              #comment out if your model contains a sigmoid or equivalent activation layer\n","              inputs = F.sigmoid(inputs)       \n","              \n","              #flatten label and prediction tensors\n","              inputs = inputs.view(-1)\n","              targets = targets.view(-1)\n","              \n","              #first compute binary cross-entropy \n","              BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","              BCE_EXP = torch.exp(-BCE)\n","              focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n","                            \n","              return focal_loss\n","      loss_fn = FocalLoss().to(DEVICE)\n","      filename=\"FocalLossModelA\"+str(ALPHA)+\"resnetEbaneo\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","      print(\"Focal loss selected\")\n","      print(filename)\n","elif lossFunction==4:\n","    #focal tversky loss\n","    #includes the gamma correction from focal loss\n","    ALPHA = 0.2 #penalizes FP \n","    BETA = 0.8 #penalizes FN\n","    GAMMA = 2\n","    LEARNING_RATE = 5e-5\n","    alpha=ALPHA\n","    betha=1-alpha\n","    class FocalTverskyLoss(nn.Module):\n","        def __init__(self, weight=None, size_average=True):\n","            super(FocalTverskyLoss, self).__init__()\n","\n","        def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n","            \n","            #comment out if your model contains a sigmoid or equivalent activation layer\n","            inputs = F.sigmoid(inputs)       \n","            \n","            #flatten label and prediction tensors\n","            inputs = inputs.view(-1)\n","            targets = targets.view(-1)\n","            \n","            #True Positives, False Positives & False Negatives\n","            TP = (inputs * targets).sum()    \n","            FP = ((1-targets) * inputs).sum()\n","            FN = (targets * (1-inputs)).sum()\n","            \n","            Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n","            FocalTversky = (1 - Tversky)**gamma\n","                          \n","            return FocalTversky\n","    loss_fn = FocalTverskyLoss().to(DEVICE)\n","    filename=\"FocalTverskyLossModel\"+str(alpha)+str(betha)+\"gamma\"+str(GAMMA)+\"resnetEbaneo\"+str(UnetDepth)+\".pth.tar\"\n","    print(\"tversky loss selected\"+str(alpha)+str(betha)+str(GAMMA))\n","    print(filename)\n","\n","else:\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    filename=\"BCEWithLogitsLossLossModelSuperAugmentedresnetEbaneo\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","    print('bcewithlogits')\n","    print(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SHYX9pVTW_f","outputId":"d72e0578-10d6-4377-fa72-fa3440a1c5d3"},"source":["#Hard Exudates\n","#diseaseType=1\n","\n","#Hemorrhages\n","#diseaseType=2\n","\n","#Soft Exudates\n","diseaseType=3\n","\n","#Microa.\n","#diseaseType=4\n","\n","root = '/content/drive/MyDrive/Colab Notebooks/'\n","folder=''\n","diseaseFolder=''\n","\n","\n","if diseaseType==1:  #for Hard exudates\n","  %cd /content/drive/My Drive/Colab Notebooks/hard exudates new unet\n","  diseaseFolder='hard exudates new unet'\n","\n","\n","elif diseaseType==2: #for hemorrages\n","  diseaseFolder='hemorrages full new unet'\n","  %cd /content/drive/My Drive/Colab Notebooks/hemorrages full new unet\n","\n","\n","\n","elif diseaseType==3: #for soft exudates\n","  diseaseFolder='soft exudates new unet'\n","  %cd /content/drive/My Drive/Colab Notebooks/soft exudates new unet\n","  filename='softExudates'+filename\n","else: #microaneurysms\n","  diseaseFolder='microaneurysms new unet'\n","  %cd /content/drive/My Drive/Colab Notebooks/microaneurysms new unet\n","\n","\n","folder='/content/drive/My Drive/Colab Notebooks/'+diseaseFolder\n","TRAIN_IMG_DIR = root+diseaseFolder+\"/images/training\"\n","TRAIN_MASK_DIR = root+diseaseFolder+\"/groundtruths/training\"\n","VAL_IMG_DIR = root+diseaseFolder+\"/images/test\"\n","VAL_MASK_DIR = root+diseaseFolder+\"/groundtruths/test\"\n","\n","\n","filename = \"LR\"+str(LEARNING_RATE)+(diseaseFolder+filename).replace(\" \", \"\")\n","print(filename)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/soft exudates new unet\n","LR5e-05softexudatesnewunetsoftExudatesLR5e-05softexudatesnewunetsoftExudatesLR5e-05hardexudatesnewunetLR5e-05softexudatesnewunetsoftExudatesLR5e-05hemorragesfullnewunetFocalTverskyLossModel0.20.8gamma2resnetEbaneo5.pth.tar\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1TQePT43TZfz"},"source":["from torch.utils.data import DataLoader\n","\n","def get_loaders(\n","    train_dir,\n","    train_maskdir,\n","    val_dir,\n","    val_maskdir,\n","    batch_size,\n","    train_transform,\n","    val_transform,\n","    num_workers=4,\n","    pin_memory=True,\n","):\n","    train_ds = CarvanaDataset(\n","                diseaseType,\n","        image_dir=train_dir,\n","        mask_dir=train_maskdir,\n","        transform=train_transform\n","\n","    )\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=True,\n","    )\n","\n","    val_ds = CarvanaDataset(\n","                diseaseType,\n","        image_dir=val_dir,\n","        mask_dir=val_maskdir,\n","        transform=val_transform\n","\n","    )\n","\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=False,\n","    )\n","\n","    return train_loader, val_loader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyRPlMyGTbFk"},"source":["training"]},{"cell_type":"code","metadata":{"id":"5UPe_j9uTb3W"},"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jp36OPMfTej7"},"source":["train_transform = A.Compose(\n","    [\n","        #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Rotate(limit=35, p=1.0),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.1),\n","        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=5, border_mode=0, p=0.5),\n","        A.GridDistortion(p=0.3),\n","        #A.ChannelShuffle(),\n","        A.OneOf([\n","                          A.MotionBlur(p=0.1),\n","                          A.OpticalDistortion(p=0.1)\n","                          ,A.GaussNoise(p=0.1)\n","                          ], p=0.1),\n","        A.Normalize(\n","            mean=[0.0, 0.0, 0.0],\n","            std=[1.0, 1.0, 1.0],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ],\n",")\n","val_transforms = A.Compose(\n","    [\n","        #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Normalize(\n","            mean=[0.0, 0.0, 0.0],\n","            std=[1.0, 1.0, 1.0],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ],\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWL0HkEvThoK"},"source":["import torch.optim as optim\n","\n","\n","\n","\n","train_loader, val_loader = get_loaders(\n","    TRAIN_IMG_DIR,\n","    TRAIN_MASK_DIR,\n","    VAL_IMG_DIR,\n","    VAL_MASK_DIR,\n","    BATCH_SIZE,\n","    train_transform,\n","    val_transforms,\n","    NUM_WORKERS,\n","    PIN_MEMORY,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcQzBxqixUoy"},"source":["import torch\n","def save_checkpoint(state, filename):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCvkKBRvTtqK"},"source":["optim = torch.optim.Adam(model.parameters())\n","import torch.nn as nn\n","bestLoss=1000\n","#loss function\n","#loss_fn= nn.BCEWithLogitsLoss()\n","trainingLoss=[]\n","validationLoss=[]\n","\n","n=0\n","\n","\n","for epoch in range(NUM_EPOCHS):\n","  n+=1\n","  print('epoch: ',n)\n","  tloss=training_function(train_loader,model,optim,loss_fn)\n","  print('tloss='+str(tloss))\n","  trainingLoss.append(tloss)\n","  vloss=validating_function(val_loader,model,optim,loss_fn)\n","  validationLoss.append(vloss)\n","  print('vloss: '+ str(vloss))\n","  if bestLoss> vloss:\n","    bestLoss=vloss\n","    checkpoint = {\n","      \"state_dict\": model.state_dict(),\n","      \"optimizer\":optimizer.state_dict(),\n","    }\n","    save_checkpoint(checkpoint,filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3taqxHr_j0gG"},"source":["def save_predictions_as_imgs(\n","    loader, model, folder=\"saved_images/\", device=\"cuda\"\n","):\n","    model.eval()\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            preds=model(x)\n","            print(torch.unique(preds))\n","            preds = torch.sigmoid(preds)\n","            preds[preds>0.5] = 1.0\n","            preds[preds<=0.5] = 0.0\n","            pre = 255.0 * np.array(preds.permute(0, 2, 3, 1).to(\"cpu\").detach())\n","            print(pre.shape)\n","            print(\"Now starting\")\n","            for i in range(BATCH_SIZE):\n","              print(pre[i].shape)\n","              abc= \"/content/drive/My Drive/softexudates/pred_\"+str(idx)+\" value \"+str(i)+\".jpg\"\n","              writeStatus = cv2.imwrite(abc, pre[i])\n","              if writeStatus is True:\n","                  print(\"image written\")\n","              else:\n","                  print(\"problem\") # or raise exception, handle problem, etc."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dJThYHZj9F-"},"source":["load_checkpoint(torch.load(\"/content/drive/MyDrive/Colab Notebooks/soft exudates new unet/newUNet_SE_FocalTverskyLossModel0.20.82UnetEbaneo.pth.tar\"), model)\n","save_predictions_as_imgs(\n","        val_loader, model, folder=\"/content/drive/MyDrive/Colab Notebooks/saved_images\", device=DEVICE\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYQ3ptjsg2C-"},"source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support, auc, precision_recall_curve\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uh_kcCMHU1Yn"},"source":["def evaluate_precission_recall(\n","    loader, model, device=\"cuda\"\n","):\n","    \n","    model.eval()\n","    predictions_array=[]\n","    targets_array=[]\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            targets = y.cpu().data.numpy()\n","            targets = torch.from_numpy(targets)\n","            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n","            #print('targets shape: ',targets.shape)\n","            targets = targets.cpu().data.numpy()\n","            targets= targets.astype(np.float16)\n","            preds=model(x)\n","            #print('preds no pad',preds.shape)\n","            #preds= F.pad(input=preds, pad=(0, 1, 0, 1), mode='constant', value=0) # 4 layers\n","            #preds= F.pad(input=preds, pad=(0, 10, 8, 2), mode='constant', value=0) # resnet 50\n","            preds= F.pad(input=preds, pad=(0, 9, 8, 1), mode='constant', value=0) # 5 layers\n","            #print('preds padd ',preds.shape)\n","            preds = torch.sigmoid(preds)\n","            preds =preds.cpu().data.numpy()\n","            preds = preds.astype(np.float16)\n","\n","            \n","            predictions_array=np.concatenate((predictions_array, preds.flatten()), axis=None)\n","            targets_array=np.concatenate((targets_array, targets.flatten()), axis=None)\n","\n","\n","    precision, recall, th = precision_recall_curve(targets_array,predictions_array)\n","    return precision, recall, th"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bv09is3o8RuR"},"source":["def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqMthPFx3zZS"},"source":["load_checkpoint(torch.load(\"/content/drive/MyDrive/Colab Notebooks/microaneurysms new unet/LR1e-06microaneurysmsnewunetFocalTverskyLossModel0.20.8gamma2resnetEbaneo5.pth.tar\"), model)\n","a,b,c = evaluate_precission_recall(val_loader,model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJBksnWfY9jH"},"source":["precision =a\n","recall =b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xC66dCC9c1pN"},"source":["a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqnTxpf3Y5mk"},"source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support, auc, precision_recall_curve\n","import matplotlib.pyplot as plt\n","\n","plt.figure()\n","plt.plot(recall,precision)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('AUPRC Microaneurysm')\n","plt.savefig('ma_focal_AUPRC_08_02_2', dpi=300, bbox_inches='tight')\n","plt.show()\n","print(f'AUC Precision Recall {str}: {auc(recall, precision)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rn6um2ePWuLo"},"source":["from torchsummary import summary\n","model = UNetWithResnet50Encoder(n_classes=1).cuda()\n","summary(model, input_size=(3, 4,569, 569))"],"execution_count":null,"outputs":[]}]}