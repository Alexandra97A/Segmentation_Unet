{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIKn1ko6MgmW","outputId":"6674d2c1-6f34-4aeb-a4d1-1c814dae103a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSRNLszWNNQy","outputId":"5029e8f2-1866-4981-891a-c940983d5ff1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting albumentations\n","  Using cached https://files.pythonhosted.org/packages/e7/27/2fa0ec5e0c04c410cbb54dd79910afa884409440653aa4688654e6497e2a/albumentations-1.0.2-py3-none-any.whl\n","Installing collected packages: albumentations\n","  Found existing installation: albumentations 1.0.2\n","    Uninstalling albumentations-1.0.2:\n","      Successfully uninstalled albumentations-1.0.2\n","Successfully installed albumentations-1.0.2\n"]}],"source":["!pip install --upgrade --force-reinstall --no-deps albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62VoUiBwL1FZ"},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms.functional as TF\n","\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","\n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","import cv2\n","from os import path\n","import torch\n","from skimage.io import imread\n","from torch.utils import data"]},{"cell_type":"markdown","metadata":{"id":"ifhje2cIlVvt"},"source":["# **Unet Architecture**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTDI5lZDdD7T"},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels=3,\n","        n_classes=1,\n","        depth=4,\n","        wf=6,\n","        padding=False,\n","        batch_norm=False,\n","        up_mode='upconv',\n","    ):\n","        \"\"\"\n","        Implementation of\n","        U-Net: Convolutional Networks for Biomedical Image Segmentation\n","        (Ronneberger et al., 2015)\n","        https://arxiv.org/abs/1505.04597\n","        Using the default arguments will yield the exact version used\n","        in the original paper\n","        Args:\n","            in_channels (int): number of input channels\n","            n_classes (int): number of output channels\n","            depth (int): depth of the network\n","            wf (int): number of filters in the first layer is 2**wf\n","            padding (bool): if True, apply padding such that the input shape\n","                            is the same as the output.\n","                            This may introduce artifacts\n","            batch_norm (bool): Use BatchNorm after layers with an\n","                               activation function\n","            up_mode (str): one of 'upconv' or 'upsample'.\n","                           'upconv' will use transposed convolutions for\n","                           learned upsampling.\n","                           'upsample' will use bilinear upsampling.\n","        \"\"\"\n","        super(UNet, self).__init__()\n","        assert up_mode in ('upconv', 'upsample')\n","        self.padding = padding\n","        self.depth = depth\n","        prev_channels = in_channels\n","        self.down_path = nn.ModuleList()\n","        for i in range(depth):\n","            self.down_path.append(\n","                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n","            )\n","            prev_channels = 2 ** (wf + i)\n","\n","        self.up_path = nn.ModuleList()\n","        for i in reversed(range(depth - 1)):\n","            self.up_path.append(\n","                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n","            )\n","            prev_channels = 2 ** (wf + i)\n","\n","        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        blocks = []\n","        for i, down in enumerate(self.down_path):\n","            x = down(x)\n","            if i != len(self.down_path) - 1:\n","                blocks.append(x)\n","                x = F.max_pool2d(x, 2)\n","\n","        for i, up in enumerate(self.up_path):\n","            x = up(x, blocks[-i - 1])\n","\n","        return self.last(x)\n","\n","\n","class UNetConvBlock(nn.Module):\n","    def __init__(self, in_size, out_size, padding, batch_norm):\n","        super(UNetConvBlock, self).__init__()\n","        block = []\n","\n","        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n","        block.append(nn.ReLU())\n","        if batch_norm:\n","            block.append(nn.BatchNorm2d(out_size))\n","\n","        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n","        block.append(nn.ReLU())\n","        if batch_norm:\n","            block.append(nn.BatchNorm2d(out_size))\n","\n","        self.block = nn.Sequential(*block)\n","\n","    def forward(self, x):\n","        out = self.block(x)\n","        return out\n","\n","\n","class UNetUpBlock(nn.Module):\n","    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n","        super(UNetUpBlock, self).__init__()\n","        if up_mode == 'upconv':\n","            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n","        elif up_mode == 'upsample':\n","            self.up = nn.Sequential(\n","                nn.Upsample(mode='bilinear', scale_factor=2),\n","                nn.Conv2d(in_size, out_size, kernel_size=1),\n","            )\n","\n","        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n","\n","    def center_crop(self, layer, target_size):\n","        _, _, layer_height, layer_width = layer.size()\n","        diff_y = (layer_height - target_size[0]) // 2\n","        diff_x = (layer_width - target_size[1]) // 2\n","        return layer[\n","            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n","        ]\n","\n","    def forward(self, x, bridge):\n","        up = self.up(x)\n","        crop1 = self.center_crop(bridge, up.shape[2:])\n","        out = torch.cat([up, crop1], 1)\n","        out = self.conv_block(out)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"J2y9CashpE4k"},"source":["# Train and validation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhFlt1TjpBi-"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gR0JF0I3lmIB"},"outputs":[],"source":["def training_function(loader, model, optimizer, loss_fn):\n","  loop = tqdm(loader)\n","  trainingLossSum=0\n","  for batch_idx, (data, targets) in enumerate(loop):\n","      data = data.to(device=DEVICE)     \n","      targets = targets.float().to(device=DEVICE)\n","      predictions = model(data)#.permute(0, 1, 3,2)\n","      predictions= F.pad(input=predictions, pad=(0, 10, 8, 2), mode='constant', value=0)\n","      train_loss = loss_fn(predictions, targets)\n","      trainingLossSum+=train_loss                  \n","      optimizer.zero_grad()\n","      train_loss.backward()\n","      optimizer.step()\n","      # update tqdm loop\n","      loop.set_postfix(loss=train_loss.item())\n","  return trainingLossSum/len(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdvspNnpnPrr"},"outputs":[],"source":["def validating_function(loader,model,optimizer,loss_fn):\n","  loop = tqdm(loader)\n","  valLossSum=0\n","  for batch_idx, (data, targets) in enumerate(loop):\n","      data = data.to(device=DEVICE)\n","      targets = targets.float().to(device=DEVICE)\n","      with torch.no_grad(): # <1> dont build graph on validation\n","              predictions = model(data)\n","              predictions = F.pad(input=predictions, pad=(0, 10, 8, 2), mode='constant', value=0)\n","              val_loss = loss_fn(predictions, targets)\n","              valLossSum+=  val_loss   \n","              assert val_loss.requires_grad == False # <2> checks that our output requires_grad\n","              # args are forced to False inside this block\n","               \n","      optimizer.zero_grad()\n","      optimizer.step()\n","       # update tqdm loop\n","      loop.set_postfix(loss=val_loss.item())\n","  return valLossSum/len(loader)"]},{"cell_type":"markdown","metadata":{"id":"6GYGP3yLpoHi"},"source":["Preparing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIwl63bmiSg6"},"outputs":[],"source":["from torchvision import transforms\n","to_tensor = transforms.ToTensor()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yp6qEi2BpqD5"},"outputs":[],"source":["def replaceInputName(mask_path,diseaseType):\n","  result = mask_path.index('IDRiD')\n","  newString=''\n","\n","  if diseaseType==1:  #for Hard exudates\n","    newString= '_EX'\n","    \n","  elif diseaseType==2: #for hemorrages\n","    newString= '_HE'\n","  elif diseaseType==3:  \n","    newString= '_SE'\n","  else:\n","    newString= '_MA'\n","\n","  toReplace= mask_path[result:result+8]\n","  mask_path=mask_path.replace(toReplace, toReplace+newString)\n","  mask_path=mask_path.replace('images','groundtruths')\n","  return mask_path\n","\n","class CarvanaDataset(Dataset):\n","   \n","    def __init__(self, diseaseType,image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","        self.diseaseType= diseaseType\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = replaceInputName(img_path,diseaseType)\n","\n","\n","        image= cv2.imread(img_path)\n","        image = np.array(image, dtype=np.float32)\n","        \n","        mask = cv2.imread(mask_path)\n","        mask = mask[:,:,2]\n","        mask[mask == 255.0] = 1.0\n","\n","\n","        #DELETE\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            #check to do augmentation IF and only IF the mask has 1 on it\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","            #check to do augmentation IF and only IF the mask has 1 on it\n","        #DELETE\n","        return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wtRoXidqg_q"},"outputs":[],"source":["# !pip install --upgrade --force-reinstall --no-deps albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mieH9EwkIijp","outputId":"32b88e97-564c-4d15-81eb-24c03d108aa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Learning rate is  5e-05\n"]}],"source":["LEARNING_RATE = 5e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Learning rate is \", LEARNING_RATE)\n","import torch.optim as optim\n","device=DEVICE\n","filename=''\n","BATCH_SIZE =4\n","NUM_EPOCHS = 500\n","NUM_WORKERS = 1\n","UnetDepth=5\n","model = UNet(n_classes=1, padding=True,batch_norm=True, up_mode='upsample',depth=UnetDepth).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","PIN_MEMORY = True\n","LOAD_MODEL = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whO8bPGNILhh"},"outputs":[],"source":["lossFunction=4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0s1FgPD3H5_w"},"outputs":[],"source":["LEARNING_RATE=0\n","if lossFunction==1:\n","    LEARNING_RATE = 5e-5\n","    ALPHA = 0.5\n","    BETA = 0.5\n","    # https://arxiv.org/abs/1706.05721\n","    #optimise segmentation on imbalanced medical datasets\n","    # in the case of α=β=0.5 the Tversky index simplifies to be the same as the Dice coefficient, which is also equal to the F1 score\n","    #modify beta increasing, so FN are punished harder\n","    class TverskyLoss(nn.Module):\n","        def __init__(self, weight=None, size_average=True):\n","            super(TverskyLoss, self).__init__()\n","\n","        def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n","            \n","            #comment out if your model contains a sigmoid or equivalent activation layer\n","            inputs = F.sigmoid(inputs)       \n","            \n","            #flatten label and prediction tensors\n","            inputs = inputs.view(-1)\n","            targets = targets.view(-1)\n","            \n","            #True Positives, False Positives & False Negatives\n","            TP = (inputs * targets).sum()    \n","            FP = ((1-targets) * inputs).sum()\n","            FN = (targets * (1-inputs)).sum()\n","          \n","            Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n","            \n","            return 1 - Tversky\n","    loss_fn = TverskyLoss().to(DEVICE)\n","    filename=\"TverskyLossModel\"+str(ALPHA)+str(BETA)+\"UnetEbaneo\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","    print(\"TverskyLoss selected\")\n","    print(filename)\n","elif lossFunction==2:\n","      LEARNING_RATE = 5e-5\n","      #combo loss :  https://arxiv.org/abs/1805.02798\n","      #Combo loss is a combination of Dice Loss and a modified Cross-Entropy function that,\n","      # like Tversky loss, has additional constants which penalise either false positives or false negatives more respectively.\n","      ##PyTorch\n","      ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\n","      CE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n","      e=1e-7\n","      class ComboLoss(nn.Module):\n","        def __init__(self, weight=None, size_average=True):\n","            super(ComboLoss, self).__init__()\n","\n","        def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n","            \n","            #flatten label and prediction tensors\n","            inputs = inputs.view(-1)\n","            targets = targets.view(-1)\n","            \n","            #True Positives, False Positives & False Negatives\n","            intersection = (inputs * targets).sum()    \n","            dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n","            \n","            inputs = torch.clamp(inputs, e, 1.0 - e)       \n","            out = - (ALPHA * ((targets * torch.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * torch.log(1.0 - inputs))))\n","            weighted_ce = out.mean(-1)\n","            combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n","            \n","            return combo\n","      loss_fn = ComboLoss().to(DEVICE)\n","      filename=\"ComboLossModel\"+str(alpha)+str(1-alpha)+\"UnetEbaneo\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","      print(\"Combo loss selected\")\n","      print(filename)\n","elif lossFunction == 3:\n","      LEARNING_RATE = 5e-5\n","      #Focal Loss\n","      #for combatting extremely imbalanced datasets https://arxiv.org/abs/1708.02002\n","\n","      ALPHA = 0.8 #DONT MODIFY\n","      GAMMA = 2  #DONT MODIFY\n","\n","      class FocalLoss(nn.Module):\n","          def __init__(self, weight=None, size_average=True):\n","              super(FocalLoss, self).__init__()\n","\n","          def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n","              \n","              #comment out if your model contains a sigmoid or equivalent activation layer\n","              inputs = F.sigmoid(inputs)       \n","              \n","              #flatten label and prediction tensors\n","              inputs = inputs.view(-1)\n","              targets = targets.view(-1)\n","              \n","              #first compute binary cross-entropy \n","              BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","              BCE_EXP = torch.exp(-BCE)\n","              focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n","                            \n","              return focal_loss\n","      loss_fn = FocalLoss().to(DEVICE)\n","      filename=\"FocalLossModelA\"+str(ALPHA)+\"Unet\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","      print(\"Focal loss selected\")\n","      print(filename)\n","elif lossFunction==4:\n","    #focal tversky loss\n","    #includes the gamma correction from focal loss\n","    ALPHA = 0.8 #penalizes FP \n","    BETA = 0.2 #penalizes FN\n","    GAMMA = 2\n","    LEARNING_RATE = 5e-5\n","    alpha=ALPHA\n","    beta=BETA\n","    class FocalTverskyLoss(nn.Module):\n","        def __init__(self, weight=None, size_average=True):\n","            super(FocalTverskyLoss, self).__init__()\n","\n","        def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n","            \n","            #comment out if your model contains a sigmoid or equivalent activation layer\n","            inputs = F.sigmoid(inputs)       \n","            \n","            #flatten label and prediction tensors\n","            inputs = inputs.view(-1)\n","            targets = targets.view(-1)\n","            \n","            #True Positives, False Positives & False Negatives\n","            TP = (inputs * targets).sum()    \n","            FP = ((1-targets) * inputs).sum()\n","            FN = (targets * (1-inputs)).sum()\n","            \n","            Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n","            FocalTversky = (1 - Tversky)**gamma\n","                          \n","            return FocalTversky\n","    loss_fn = FocalTverskyLoss().to(DEVICE)\n","    filename=\"FocalTverskyLossModel\"+str(alpha)+str(beta)+\"gamma\"+str(GAMMA)+\"Unet\"+str(UnetDepth)+\".pth.tar\"\n","    print(\"tversky loss selected\"+str(alpha)+str(beta)+str(GAMMA))\n","    print(filename)\n","\n","else:\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    filename=\"BCEWithLogitsLossLossModelSuperAugmentedUnet\"+\"DEPTH\"+str(UnetDepth)+\".pth.tar\"\n","    print('bcewithlogits')\n","    print(filename)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8yc77B1tI71"},"outputs":[],"source":["# #Hard Exudate\n","#diseaseType=1\n","\n","#HEmo\n","diseaseType=2\n","\n","# #soft exudates\n","#diseaseType=3\n","\n","# #micro an\n","#diseaseType=4\n","\n","root = '/content/drive/MyDrive/Colab Notebooks/'\n","folder=''\n","diseaseFolder=''\n","\n","if diseaseType==1:  #for Hard exudates\n","  %cd /content/drive/My Drive/Colab Notebooks/hard exudates new unet\n","  diseaseFolder='hard exudates new unet'\n","  \n","elif diseaseType==2: #for hemorrages\n","  diseaseFolder='hemorrages full new unet'\n","  %cd /content/drive/My Drive/Colab Notebooks/hemorrages full new unet\n","\n","elif diseaseType==3: #for soft exudates\n","  diseaseFolder='soft exudates new unet'\n","  %cd /content/drive/My Drive/Colab Notebooks/soft exudates new unet\n","\n","else: #microaneurysms\n","  diseaseFolder='microaneurysms new unet sliced'\n","  %cd /content/drive/My Drive/Colab Notebooks/microaneurysms new unet sliced\n","\n","folder='/content/drive/My Drive/Colab Notebooks/'+diseaseFolder\n","TRAIN_IMG_DIR = root+diseaseFolder+\"/images/training\"\n","TRAIN_MASK_DIR = root+diseaseFolder+\"/groundtruths/training\"\n","VAL_IMG_DIR = root+diseaseFolder+\"/images/test\"\n","VAL_MASK_DIR = root+diseaseFolder+\"/groundtruths/test\"\n","\n","filename = \"LR\"+str(LEARNING_RATE)+(diseaseFolder+filename).replace(\" \", \"\")\n","print(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CNhVIbVpyIv"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def get_loaders(\n","    train_dir,\n","    train_maskdir,\n","    val_dir,\n","    val_maskdir,\n","    batch_size,\n","    train_transform,\n","    val_transform,\n","    num_workers=4,\n","    pin_memory=True,\n","):\n","    train_ds = CarvanaDataset(\n","                diseaseType,\n","        image_dir=train_dir,\n","        mask_dir=train_maskdir,\n","        transform=train_transform\n","\n","    )\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=True,\n","    )\n","\n","    val_ds = CarvanaDataset(\n","                diseaseType,\n","        image_dir=val_dir,\n","        mask_dir=val_maskdir,\n","        transform=val_transform\n","\n","    )\n","\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=False,\n","    )\n","\n","    return train_loader, val_loader"]},{"cell_type":"markdown","metadata":{"id":"ccrn7PkyqwPc"},"source":["Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZOCUFzwsCcy"},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mOY5ZuyoWqU"},"outputs":[],"source":["train_transform = A.Compose(\n","    [\n","        A.Rotate(limit=35, p=1.0),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.1),\n","        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=5, border_mode=0, p=0.5),\n","        A.GridDistortion(p=0.3),\n","        #A.ChannelShuffle(),\n","        A.OneOf([\n","                          A.MotionBlur(p=0.1),\n","                          A.OpticalDistortion(p=0.1)\n","                          ,A.GaussNoise(p=0.1)\n","                          ], p=0.1),\n","        A.Normalize(\n","            mean=[0.0, 0.0, 0.0],\n","            std=[1.0, 1.0, 1.0],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ],\n",")\n","val_transforms = A.Compose(\n","    [\n","        A.Normalize(\n","            mean=[0.0, 0.0, 0.0],\n","            std=[1.0, 1.0, 1.0],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uK9iwL0zqxny"},"outputs":[],"source":["import torch.optim as optim\n","train_loader, val_loader = get_loaders(\n","    TRAIN_IMG_DIR,\n","    TRAIN_MASK_DIR,\n","    VAL_IMG_DIR,\n","    VAL_MASK_DIR,\n","    BATCH_SIZE,\n","    train_transform,\n","    val_transforms,\n","    NUM_WORKERS,\n","    PIN_MEMORY,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PpGWsolJ-AX","outputId":"1f237163-c2b7-4ce5-9308-20eb9a714ff8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.5.2)\n"]}],"source":["pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eU6RjmGDJ-nx"},"outputs":[],"source":["from torchinfo import summary\n","\n","batch_size = 4\n","summary(model, input_size=(batch_size, 3, 474, 714))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4eO1KNtrirV"},"outputs":[],"source":["def save_checkpoint(state, filename):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qeQDTYLrCoW"},"outputs":[],"source":["optim = torch.optim.Adam(model.parameters())\n","import torch.nn as nn\n","bestLoss=1000\n","#loss function\n","#loss_fn= nn.BCEWithLogitsLoss()\n","trainingLoss=[]\n","validationLoss=[]\n","\n","n=0\n","\n","for epoch in range(NUM_EPOCHS):\n","  n+=1\n","  print('epoch: ',n)\n","  tloss=training_function(train_loader,model,optim,loss_fn)\n","  print('tloss='+str(tloss))\n","  trainingLoss.append(tloss)\n","  vloss=validating_function(val_loader,model,optim,loss_fn)\n","  validationLoss.append(vloss)\n","  print('vloss: '+ str(vloss))\n","  if bestLoss> vloss:\n","    bestLoss=vloss\n","    checkpoint = {\n","      \"state_dict\": model.state_dict(),\n","      \"optimizer\":optimizer.state_dict(),\n","    }\n","    save_checkpoint(checkpoint,filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kU-ZPyRex5bk"},"outputs":[],"source":["def save_predictions_as_imgs(\n","    loader, model, folder=\"saved_images/\", device=\"cuda\"\n","):\n","    model.eval()\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            preds=model(x)\n","            print(torch.unique(preds))\n","            preds = torch.sigmoid(preds)\n","            preds[preds>0.5] = 1.0\n","            preds[preds<=0.5] = 0.0\n","            pre = 255.0 * np.array(preds.permute(0, 2, 3, 1).to(\"cpu\").detach())\n","            print(pre.shape)\n","            print(\"Now starting\")\n","            for i in range(BATCH_SIZE):\n","              print(pre[i].shape)\n","              abc= \"/content/drive/My Drive/hemorrages08/pred_\"+str(idx)+\" value \"+str(i)+\".jpg\"\n","              writeStatus = cv2.imwrite(abc, pre[i])\n","              if writeStatus is True:\n","                  print(\"image written\")\n","              else:\n","                  print(\"problem\") # or raise exception, handle problem, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ge_b6VS3z62"},"outputs":[],"source":["#load_checkpoint(torch.load(\"TverskyLossModel0.50.5UnetEbaneoDEPTH5.pth.tar\"), model)\n","save_predictions_as_imgs(\n","        val_loader, model, folder=\"/content/drive/MyDrive/Colab Notebooks/saved_images\", device=DEVICE\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTQLKHWj8q5Y"},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support, auc, precision_recall_curve\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3PVVNYkG5YJ"},"outputs":[],"source":["def evaluate_precission_recall(\n","    loader, model, device=\"cuda\"\n","):\n","    \n","    model.eval()\n","    predictions_array=[]\n","    targets_array=[]\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            targets = y.cpu().data.numpy()\n","            targets = torch.from_numpy(targets)\n","            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n","            targets = targets.cpu().data.numpy()\n","            targets= targets.astype(np.float16)\n","            preds=model(x)\n","            #preds= F.pad(input=preds, pad=(0, 1, 0, 1), mode='constant', value=0) # 4 layers\n","            #preds= F.pad(input=preds, pad=(0, 10, 8, 2), mode='constant', value=0) # resnet 50\n","            preds= F.pad(input=preds, pad=(0, 10, 9, 1), mode='constant', value=0) # 5 layers\n","            #print('preds padd ',preds.shape)\n","            preds = torch.sigmoid(preds)\n","            preds =preds.cpu().data.numpy()\n","            preds = preds.astype(np.float16)\n","\n","            \n","            predictions_array=np.concatenate((predictions_array, preds.flatten()), axis=None)\n","            targets_array=np.concatenate((targets_array, targets.flatten()), axis=None)\n","\n","\n","    precision, recall, th = precision_recall_curve(targets_array,predictions_array)\n","    return precision, recall, th"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWXcrJDiIc4Q"},"outputs":[],"source":["# load_checkpoint(torch.load(\"/content/drive/MyDrive/Colab Notebooks/hemorrages full new unet/LR5e-05hemorragesfullnewunetFocalTverskyLossModel0.20.8gamma2resnetEbaneo5.pth.tar\"), model)\n","a,b,c = evaluate_precission_recall(val_loader,model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Odc6RNm_xUri"},"outputs":[],"source":["precision =a\n","recall =b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBBXv0KqHwRR"},"outputs":[],"source":["plt.figure()\n","plt.plot(recall,precision)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('AUPRC Microaneurysm')\n","plt.savefig('ma_focal_AUPRC_08_02_2', dpi=300, bbox_inches='tight')\n","plt.show()\n","print(f'AUC Precision Recall {str}: {auc(recall, precision)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrCoOQNRxXjM"},"outputs":[],"source":["x = torch.randn(4, 857, 569, 3)\n","x.size()\n","#x=F.pad(input=x, pad=(1, 0, 1, 0), mode='constant', value=0)\n","x = x.permute(0,3, 2,1)\n","x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkNnQcu02vis"},"outputs":[],"source":["#load_checkpoint(torch.load(\"TverskyLossModel0.50.5UnetEbaneoDEPTH5.pth.tar\"), model)\n","for batch_idx, (data, targets) in enumerate(loop):\n","\n","    data = data.to(device=DEVICE)\n","    targets = targets.float().to(device=DEVICE)\n","    predictions = model(data)#.permute(0, 1, 3,2)\n","    preds = torch.sigmoid(predictions)\n","    preds[preds>0.5] = 1.0\n","    preds[preds<=0.5] = 0.0\n","    #groundtruth\n","\n","    precision, recall, thresholds = precision_recall_curve(targets, preds)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"unet_flexible.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
